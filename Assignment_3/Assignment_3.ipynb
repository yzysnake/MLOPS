{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef0d492bc596a38",
   "metadata": {},
   "source": [
    "# Assignment 3: AutoML\n",
    "## Peter Ye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0fa38d4ce08557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:43.655445Z",
     "start_time": "2024-11-13T04:06:43.484807Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7feedf85e42cca60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:44.720353Z",
     "start_time": "2024-11-13T04:06:43.884287Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the original CSV\n",
    "athletes_df = pd.read_csv('athletes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad833795ed813a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Pre-processing to create different feature views to simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b50c1d7035d50c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:44.836385Z",
     "start_time": "2024-11-13T04:06:44.778417Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Dropping rows with null values\n",
    "athletes_df = athletes_df.dropna(subset=['region','age','weight','height','howlong','gender','eat',\n",
    "                               'train','background','experience','schedule','howlong',\n",
    "                               'deadlift','candj','snatch','backsq','experience',\n",
    "                               'background','schedule','howlong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58666a061dd55454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:44.941193Z",
     "start_time": "2024-11-13T04:06:44.940636Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns\n",
    "athletes_df = athletes_df.drop(columns=['affiliate','team','name','fran','helen','grace',\n",
    "                              'filthy50','fgonebad','run400','run5k','pullups','train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5637cde75aaf52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:45.114437Z",
     "start_time": "2024-11-13T04:06:45.089383Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "athletes_df = athletes_df[athletes_df['weight'] < 1500]\n",
    "athletes_df = athletes_df[athletes_df['gender'] != '--']\n",
    "athletes_df = athletes_df[athletes_df['age'] >= 18]\n",
    "athletes_df = athletes_df[(athletes_df['height'] < 96) & (athletes_df['height'] > 48)]\n",
    "athletes_df = athletes_df[(athletes_df['deadlift'] > 0) & \n",
    "                ((athletes_df['deadlift'] <= 1105) | \n",
    "                ((athletes_df['gender'] == 'Female') & (athletes_df['deadlift'] <= 636)))]\n",
    "athletes_df = athletes_df[(athletes_df['candj'] > 0) & (athletes_df['candj'] <= 395)]\n",
    "athletes_df = athletes_df[(athletes_df['snatch'] > 0) & (athletes_df['snatch'] <= 496)]\n",
    "athletes_df = athletes_df[(athletes_df['backsq'] > 0) & (athletes_df['backsq'] <= 1069)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "417a15abed8fae1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:45.307087Z",
     "start_time": "2024-11-13T04:06:45.262389Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Clean survey data\n",
    "decline_dict = {'Decline to answer|': np.nan}\n",
    "athletes_df = athletes_df.replace(decline_dict)\n",
    "athletes_df = athletes_df.dropna(subset=['background','experience','schedule','howlong','eat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee11bb16bf6934b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:45.491307Z",
     "start_time": "2024-11-13T04:06:45.439472Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create 'total_lift' column\n",
    "athletes_df['total_lift'] = athletes_df['candj'] + athletes_df['snatch'] + athletes_df['deadlift'] + athletes_df['backsq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d821acde269536af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:45.639314Z",
     "start_time": "2024-11-13T04:06:45.588268Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>athlete_id</th>\n",
       "      <th>region</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>candj</th>\n",
       "      <th>snatch</th>\n",
       "      <th>deadlift</th>\n",
       "      <th>backsq</th>\n",
       "      <th>eat</th>\n",
       "      <th>background</th>\n",
       "      <th>experience</th>\n",
       "      <th>schedule</th>\n",
       "      <th>howlong</th>\n",
       "      <th>total_lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21269.0</td>\n",
       "      <td>Southern California</td>\n",
       "      <td>Male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>I eat whatever is convenient|</td>\n",
       "      <td>I played youth or high school level sports|I p...</td>\n",
       "      <td>I began CrossFit by trying it alone (without a...</td>\n",
       "      <td>I do multiple workouts in a day 1x a week|I ty...</td>\n",
       "      <td>1-2 years|</td>\n",
       "      <td>1110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21685.0</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>I eat 1-3 full cheat meals per week|</td>\n",
       "      <td>I have no athletic background besides CrossFit|</td>\n",
       "      <td>I began CrossFit with a coach (e.g. at an affi...</td>\n",
       "      <td>I do multiple workouts in a day 1x a week|</td>\n",
       "      <td>2-4 years|</td>\n",
       "      <td>910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25464.0</td>\n",
       "      <td>North East</td>\n",
       "      <td>Male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>I eat quality foods but don't measure the amount|</td>\n",
       "      <td>I played youth or high school level sports|</td>\n",
       "      <td>I began CrossFit with a coach (e.g. at an affi...</td>\n",
       "      <td>I typically rest 4 or more days per month|</td>\n",
       "      <td>2-4 years|</td>\n",
       "      <td>1335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>43767.0</td>\n",
       "      <td>North Central</td>\n",
       "      <td>Male</td>\n",
       "      <td>36.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>I eat quality foods but don't measure the amount|</td>\n",
       "      <td>I played youth or high school level sports|I p...</td>\n",
       "      <td>I began CrossFit with a coach (e.g. at an affi...</td>\n",
       "      <td>I do multiple workouts in a day 3+ times a wee...</td>\n",
       "      <td>1-2 years|</td>\n",
       "      <td>1354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>55504.0</td>\n",
       "      <td>North East</td>\n",
       "      <td>Male</td>\n",
       "      <td>36.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>I eat strict Paleo|</td>\n",
       "      <td>I played youth or high school level sports|I p...</td>\n",
       "      <td>I began CrossFit by trying it alone (without a...</td>\n",
       "      <td>I do multiple workouts in a day 2x a week|I st...</td>\n",
       "      <td>4+ years|</td>\n",
       "      <td>1225.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    athlete_id               region gender   age  height  weight  candj  \\\n",
       "21     21269.0  Southern California   Male  30.0    71.0   200.0  235.0   \n",
       "22     21685.0               Africa   Male  28.0    70.0   176.0  187.0   \n",
       "27     25464.0           North East   Male  35.0    68.0   225.0  285.0   \n",
       "50     43767.0        North Central   Male  36.0    71.0   199.0  267.0   \n",
       "60     55504.0           North East   Male  36.0    64.0   155.0  245.0   \n",
       "\n",
       "    snatch  deadlift  backsq  \\\n",
       "21   175.0     385.0   315.0   \n",
       "22   134.0     335.0   254.0   \n",
       "27   205.0     440.0   405.0   \n",
       "50   212.0     485.0   390.0   \n",
       "60   180.0     415.0   385.0   \n",
       "\n",
       "                                                  eat  \\\n",
       "21                      I eat whatever is convenient|   \n",
       "22               I eat 1-3 full cheat meals per week|   \n",
       "27  I eat quality foods but don't measure the amount|   \n",
       "50  I eat quality foods but don't measure the amount|   \n",
       "60                                I eat strict Paleo|   \n",
       "\n",
       "                                           background  \\\n",
       "21  I played youth or high school level sports|I p...   \n",
       "22    I have no athletic background besides CrossFit|   \n",
       "27        I played youth or high school level sports|   \n",
       "50  I played youth or high school level sports|I p...   \n",
       "60  I played youth or high school level sports|I p...   \n",
       "\n",
       "                                           experience  \\\n",
       "21  I began CrossFit by trying it alone (without a...   \n",
       "22  I began CrossFit with a coach (e.g. at an affi...   \n",
       "27  I began CrossFit with a coach (e.g. at an affi...   \n",
       "50  I began CrossFit with a coach (e.g. at an affi...   \n",
       "60  I began CrossFit by trying it alone (without a...   \n",
       "\n",
       "                                             schedule     howlong  total_lift  \n",
       "21  I do multiple workouts in a day 1x a week|I ty...  1-2 years|      1110.0  \n",
       "22         I do multiple workouts in a day 1x a week|  2-4 years|       910.0  \n",
       "27         I typically rest 4 or more days per month|  2-4 years|      1335.0  \n",
       "50  I do multiple workouts in a day 3+ times a wee...  1-2 years|      1354.0  \n",
       "60  I do multiple workouts in a day 2x a week|I st...   4+ years|      1225.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athletes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29decafb30005e28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:45.877568Z",
     "start_time": "2024-11-13T04:06:45.778450Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Adding new columns based on the specified conditions\n",
    "athletes_df['experience_start_with_coach'] = athletes_df['experience'].apply(\n",
    "    lambda x: 1 if 'I began CrossFit with a coach' in x else 0\n",
    ")\n",
    "\n",
    "athletes_df['experience_have_certificate'] = athletes_df['experience'].apply(\n",
    "    lambda x: 1 if 'I have completed the CrossFit Level 1 certificate course' in x else 0\n",
    ")\n",
    "\n",
    "athletes_df['eat_on_diet'] = athletes_df['eat'].apply(\n",
    "    lambda x: 1 if 'I eat strict Paleo' in x else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2794721e5bfe2b26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:46.910807Z",
     "start_time": "2024-11-13T04:06:46.879201Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert gender to binary number\n",
    "athletes_df['gender'] = athletes_df['gender'].map({'Male': 1, 'Female': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d788db0fa50dab3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:47.821734Z",
     "start_time": "2024-11-13T04:06:47.575857Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select columns to normalize\n",
    "athletes_df[['age', 'height', 'weight']] = scaler.fit_transform(athletes_df[['age', 'height', 'weight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d998f3d8c310d706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:06:49.651906Z",
     "start_time": "2024-11-13T04:06:49.639444Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "athletes_df = athletes_df[['age', 'height', 'weight', 'gender', 'experience_start_with_coach', 'experience_have_certificate', 'eat_on_diet', 'total_lift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae68d310cae1a3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:17:27.038799Z",
     "start_time": "2024-11-13T04:17:26.988732Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender</th>\n",
       "      <th>experience_start_with_coach</th>\n",
       "      <th>experience_have_certificate</th>\n",
       "      <th>eat_on_diet</th>\n",
       "      <th>total_lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.415778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.364606</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.469083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.413646</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.319829</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1225.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age    height    weight  gender  experience_start_with_coach  \\\n",
       "21  0.315789  0.612903  0.415778       1                            0   \n",
       "22  0.263158  0.580645  0.364606       1                            1   \n",
       "27  0.447368  0.516129  0.469083       1                            1   \n",
       "50  0.473684  0.612903  0.413646       1                            1   \n",
       "60  0.473684  0.387097  0.319829       1                            0   \n",
       "\n",
       "    experience_have_certificate  eat_on_diet  total_lift  \n",
       "21                            0            0      1110.0  \n",
       "22                            0            0       910.0  \n",
       "27                            0            0      1335.0  \n",
       "50                            1            0      1354.0  \n",
       "60                            1            1      1225.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athletes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff75e95d10950ead",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddda9b56d31f649f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:15:26.643234Z",
     "start_time": "2024-11-13T04:15:26.480700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b726a27a7ed2170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:25:56.101043Z",
     "start_time": "2024-11-13T04:25:56.079773Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define feature columns and target variable\n",
    "feature_columns = [\n",
    "    'age', 'height', 'weight', 'experience_start_with_coach',\n",
    "    'experience_have_certificate', 'eat_on_diet', 'gender'\n",
    "]\n",
    "target_column = 'total_lift'\n",
    "\n",
    "# Prepare data for AutoGluon\n",
    "data = athletes_df[feature_columns + [target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53232569d170c94",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2&3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "786f893c719d5a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:52:20.545118Z",
     "start_time": "2024-11-13T04:46:47.242096Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AutoGluon AutoML with all features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: OSError(\"/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/ray/_raylet.so)\")\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:50] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:50] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:51] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:51] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:51] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:52] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:52] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:53] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:53] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:54] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:54] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:55] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:55] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:56] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:56] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:57] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:57] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:22:57] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:23:47] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:23:47] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t0 epochs trained!\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:24:35] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:24:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:24:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:24:50] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:24:50] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:24:59] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:24:59] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:25:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:25:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:25:13] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:25:13] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:25:21] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:25:21] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:25:29] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:25:29] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:25:36] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(\n",
    "    data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Run AutoML with all features\n",
    "print(\"Running AutoGluon AutoML with all features...\")\n",
    "predictor = TabularPredictor(\n",
    "    label=target_column, eval_metric='r2', verbosity=0\n",
    ").fit(\n",
    "    train_data=train_data,\n",
    "    time_limit=360,  # Total time in seconds\n",
    "    presets='best_quality',\n",
    "    ag_args_fit={'num_gpus': 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5374b114e705a416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:52:50.458109Z",
     "start_time": "2024-11-13T04:52:29.163227Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Performance (All Features):\n",
      "{'r2': 0.685087073657951, 'root_mean_squared_error': np.float64(-156.5078415640652), 'mean_squared_error': np.float64(-24494.70447104253), 'mean_absolute_error': np.float64(-121.21944970304561), 'pearsonr': 0.8278431818012704, 'median_absolute_error': np.float64(-99.674560546875)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the AutoML model on the test set\n",
    "performance = predictor.evaluate(test_data)\n",
    "print(\"\\nTest Performance (All Features):\")\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97934278db8881a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dd8769472e2bd41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:56:14.849751Z",
     "start_time": "2024-11-13T04:55:00.074868Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating feature importances...\n",
      "\n",
      "Feature Importances:\n",
      "                             importance    stddev   p_value  n  p99_high  \\\n",
      "gender                         0.539779  0.126475  0.000337  5  0.800193   \n",
      "weight                         0.355942  0.053110  0.000058  5  0.465296   \n",
      "experience_have_certificate    0.063289  0.006050  0.000010  5  0.075745   \n",
      "age                            0.062452  0.009767  0.000070  5  0.082563   \n",
      "height                         0.041544  0.004845  0.000022  5  0.051521   \n",
      "experience_start_with_coach    0.006413  0.003563  0.007901  5  0.013750   \n",
      "eat_on_diet                    0.000586  0.000624  0.051834  5  0.001871   \n",
      "\n",
      "                              p99_low  \n",
      "gender                       0.279365  \n",
      "weight                       0.246588  \n",
      "experience_have_certificate  0.050832  \n",
      "age                          0.042340  \n",
      "height                       0.031567  \n",
      "experience_start_with_coach -0.000923  \n",
      "eat_on_diet                 -0.000699  \n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "print(\"\\nCalculating feature importances...\")\n",
    "feature_importance = predictor.feature_importance(test_data, subsample_size=500, silent=True)\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "588efb68ae787ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T04:57:44.192116Z",
     "start_time": "2024-11-13T04:57:44.180825Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Features: ['gender', 'weight', 'experience_have_certificate', 'age', 'height']\n"
     ]
    }
   ],
   "source": [
    "# Get the top 5 features\n",
    "top_5_features = feature_importance.index[:5].tolist()\n",
    "print(f\"\\nTop 5 Features: {top_5_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01b958e3370285",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8497c36054c60408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:00:09.287599Z",
     "start_time": "2024-11-13T05:00:09.235785Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Top 3 Features: ['gender', 'weight', 'experience_have_certificate']\n"
     ]
    }
   ],
   "source": [
    "# Prepare data using only the top 3 features\n",
    "top_3_features = feature_importance.index[:3].tolist()\n",
    "print(f\"Using Top 3 Features: {top_3_features}\")\n",
    "\n",
    "# Prepare datasets with top 3 features\n",
    "train_data_top3 = train_data[top_3_features + [target_column]]\n",
    "test_data_top3 = test_data[top_3_features + [target_column]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66679d6f19030dde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:10:00.253925Z",
     "start_time": "2024-11-13T05:04:26.998221Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running AutoGluon AutoML with top 3 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: OSError(\"/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/ray/_raylet.so)\")\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:02] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:05] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:05] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:08] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:08] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:13] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:13] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:16] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:16] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:20] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:20] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:23] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:23] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:27] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.1.1`. \n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:17] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:32] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:32] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:40] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:40] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:47] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:47] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:54] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:54] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:00] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:01] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:14] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1730232887822/work/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n"
     ]
    }
   ],
   "source": [
    "# Run AutoML with top 3 features\n",
    "print(\"\\nRunning AutoGluon AutoML with top 3 features...\")\n",
    "predictor_top3 = TabularPredictor(\n",
    "    label=target_column, eval_metric='r2', verbosity=0\n",
    ").fit(\n",
    "    train_data=train_data_top3,\n",
    "    time_limit=360,\n",
    "    presets='best_quality',\n",
    "     ag_args_fit={'num_gpus': 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1163accb1ca1868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:10:33.395574Z",
     "start_time": "2024-11-13T05:10:11.479661Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Performance (Top 3 Features):\n",
      "{'r2': 0.6279093570625114, 'root_mean_squared_error': np.float64(-170.12386180698851), 'mean_squared_error': np.float64(-28942.12835612332), 'mean_absolute_error': np.float64(-132.35145413539973), 'pearsonr': 0.7925964433294088, 'median_absolute_error': np.float64(-109.057861328125)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the AutoML model with top 3 features on the test set\n",
    "performance_top3 = predictor_top3.evaluate(test_data_top3)\n",
    "print(\"\\nTest Performance (Top 3 Features):\")\n",
    "print(performance_top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f4d118394de1776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:11:32.126615Z",
     "start_time": "2024-11-13T05:10:44.127120Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Models per Validation Score (All Features):\n",
      "                   model  score_val\n",
      "0    WeightedEnsemble_L2   0.691112\n",
      "1  NeuralNetTorch_BAG_L1   0.689125\n",
      "2        LightGBM_BAG_L1   0.688327\n"
     ]
    }
   ],
   "source": [
    "# Extracting model leaderboard\n",
    "leaderboard = predictor.leaderboard(test_data, silent=True)\n",
    "leaderboard_top3 = predictor_top3.leaderboard(test_data_top3, silent=True)\n",
    "\n",
    "# Top 3 models per validation score (All Features)\n",
    "top3_models_score = leaderboard.sort_values(\n",
    "    by='score_val', ascending=False\n",
    ").head(3)\n",
    "print(\"\\nTop 3 Models per Validation Score (All Features):\")\n",
    "print(top3_models_score[['model', 'score_val']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5329306581bea7ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:11:46.357880Z",
     "start_time": "2024-11-13T05:11:46.308427Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Models per Validation Score (Top 3 Features):\n",
      "                 model  score_val\n",
      "0  WeightedEnsemble_L2   0.627594\n",
      "1    LightGBMXT_BAG_L1   0.627152\n",
      "2      LightGBM_BAG_L1   0.626493\n"
     ]
    }
   ],
   "source": [
    "# Top 3 models per validation score (Top 3 Features)\n",
    "top3_models_score_top3 = leaderboard_top3.sort_values(\n",
    "    by='score_val', ascending=False\n",
    ").head(3)\n",
    "print(\"\\nTop 3 Models per Validation Score (Top 3 Features):\")\n",
    "print(top3_models_score_top3[['model', 'score_val']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705218bf3a9cf92f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18578a8bebdfbf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:12:35.531001Z",
     "start_time": "2024-11-13T05:12:35.482342Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Models per Speed (All Features):\n",
      "                   model  pred_time_val\n",
      "2        LightGBM_BAG_L1       0.021980\n",
      "7  KNeighborsUnif_BAG_L1       0.051594\n",
      "3      LightGBMXT_BAG_L1       0.069449\n"
     ]
    }
   ],
   "source": [
    "# Top 3 models per speed (All Features)\n",
    "top3_models_speed = leaderboard.sort_values(\n",
    "    by='pred_time_val'\n",
    ").head(3)\n",
    "print(\"\\nTop 3 Models per Speed (All Features):\")\n",
    "print(top3_models_speed[['model', 'pred_time_val']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3329f66b4f5ff810",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:12:40.766061Z",
     "start_time": "2024-11-13T05:12:40.739826Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Models per Speed (Top 3 Features):\n",
      "                  model  pred_time_val\n",
      "2       LightGBM_BAG_L1       0.016299\n",
      "5  LightGBMLarge_BAG_L1       0.023060\n",
      "1     LightGBMXT_BAG_L1       0.044685\n"
     ]
    }
   ],
   "source": [
    "# Top 3 models per speed (Top 3 Features)\n",
    "top3_models_speed_top3 = leaderboard_top3.sort_values(\n",
    "    by='pred_time_val'\n",
    ").head(3)\n",
    "print(\"\\nTop 3 Models per Speed (Top 3 Features):\")\n",
    "print(top3_models_speed_top3[['model', 'pred_time_val']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176251e637183e48",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca35ce8baa8fb8c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "* The best model (WeightedEnsemble_L2) which has the best 0.691112 validation score outperforms previous models which only have 0.62 as best score\n",
    "* The best model (LightGBM_BAG_L1) which has the best 0.691112 prediction time can't tell a difference between previous models. The validation speed doesn't really matter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6808784b1e0c4d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595242480237a94",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The autogluon is open-source and low-code platform. From the experiment, we don't have to explicitly write down codes defining the model pipeline, category, and hyper-parameters. We just need to tell the platform what are targeted variable and explanatory variables, compared to traditional ML package like sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558a70edb94381b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90b654fd16ac7fec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:27:46.416853Z",
     "start_time": "2024-11-13T05:27:46.311404Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ddab434987ba55",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "146d9e4f76bb6c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:28:57.271156Z",
     "start_time": "2024-11-13T05:28:53.418369Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"11.0.25\" 2024-10-15; OpenJDK Runtime Environment (build 11.0.25+9-post-Ubuntu-1ubuntu122.04); OpenJDK 64-Bit Server VM (build 11.0.25+9-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n",
      "  Starting server from /home/yzysnake/miniconda3/envs/Mlop/lib/python3.9/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpj36w6dam\n",
      "  JVM stdout: /tmp/tmpj36w6dam/h2o_yzysnake_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpj36w6dam/h2o_yzysnake_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/Chicago</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.6</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>10 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_yzysnake_oq2gwv</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>7.803 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>32</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>32</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.9.20 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O_cluster_uptime:         01 secs\n",
       "H2O_cluster_timezone:       America/Chicago\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.6\n",
       "H2O_cluster_version_age:    10 days\n",
       "H2O_cluster_name:           H2O_from_python_yzysnake_oq2gwv\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    7.803 Gb\n",
       "H2O_cluster_total_cores:    32\n",
       "H2O_cluster_allowed_cores:  32\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.9.20 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c17e76611f93f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:35:17.505067Z",
     "start_time": "2024-11-13T05:29:58.473338Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: || (done) 100%\n",
      "Running H2O AutoML with all features...\n",
      "AutoML progress: || (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
       "=============\n",
       "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
       "Model Key: StackedEnsemble_AllModels_5_AutoML_1_20241112_220908\n",
       "</pre>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-2.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-2 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-2 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-2 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table th,\n",
       "#h2o-table-2 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Model Summary for Stacked Ensemble: </caption>\n",
       "    <thead><tr><th>key</th>\n",
       "<th>value</th></tr></thead>\n",
       "    <tbody><tr><td>Stacking strategy</td>\n",
       "<td>cross_validation</td></tr>\n",
       "<tr><td>Number of base models (used / total)</td>\n",
       "<td>20/44</td></tr>\n",
       "<tr><td># GBM base models (used / total)</td>\n",
       "<td>18/36</td></tr>\n",
       "<tr><td># XGBoost base models (used / total)</td>\n",
       "<td>2/5</td></tr>\n",
       "<tr><td># GLM base models (used / total)</td>\n",
       "<td>0/1</td></tr>\n",
       "<tr><td># DRF base models (used / total)</td>\n",
       "<td>0/2</td></tr>\n",
       "<tr><td>Metalearner algorithm</td>\n",
       "<td>GLM</td></tr>\n",
       "<tr><td>Metalearner fold assignment scheme</td>\n",
       "<td>Random</td></tr>\n",
       "<tr><td>Metalearner nfolds</td>\n",
       "<td>5</td></tr>\n",
       "<tr><td>Metalearner fold_column</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>Custom metalearner hyperparameters</td>\n",
       "<td>None</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 21729.345578635894\n",
       "RMSE: 147.40877035860484\n",
       "MAE: 113.87901979976012\n",
       "RMSLE: 0.16583606754264976\n",
       "Mean Residual Deviance: 21729.345578635894\n",
       "R^2: 0.7163470681389101\n",
       "Null degrees of freedom: 10018\n",
       "Residual degrees of freedom: 9998\n",
       "Null deviance: 767543465.1646143\n",
       "Residual deviance: 217706313.352353\n",
       "AIC: 128530.62185618874</pre></div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on cross-validation data. **\n",
       "\n",
       "MSE: 24087.4315931657\n",
       "RMSE: 155.20126157079298\n",
       "MAE: 119.35785763757742\n",
       "RMSLE: 0.18768092487335972\n",
       "Mean Residual Deviance: 24087.4315931657\n",
       "R^2: 0.6878424037257309\n",
       "Null degrees of freedom: 24100\n",
       "Residual degrees of freedom: 24081\n",
       "Null deviance: 1860043823.2859292\n",
       "Residual deviance: 580531188.8268865\n",
       "AIC: 311603.400517189</pre></div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-3.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-3 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-3 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-3 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-3 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-3 .h2o-table th,\n",
       "#h2o-table-3 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-3 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-3\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Cross-Validation Metrics Summary: </caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>mean</th>\n",
       "<th>sd</th>\n",
       "<th>cv_1_valid</th>\n",
       "<th>cv_2_valid</th>\n",
       "<th>cv_3_valid</th>\n",
       "<th>cv_4_valid</th>\n",
       "<th>cv_5_valid</th></tr></thead>\n",
       "    <tbody><tr><td>aic</td>\n",
       "<td>62353.59</td>\n",
       "<td>558.67554</td>\n",
       "<td>62579.92</td>\n",
       "<td>61501.004</td>\n",
       "<td>63002.85</td>\n",
       "<td>62186.293</td>\n",
       "<td>62497.87</td></tr>\n",
       "<tr><td>loglikelihood</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>mae</td>\n",
       "<td>119.41707</td>\n",
       "<td>2.0313985</td>\n",
       "<td>119.01283</td>\n",
       "<td>120.53866</td>\n",
       "<td>120.6973340</td>\n",
       "<td>116.0255</td>\n",
       "<td>120.81102</td></tr>\n",
       "<tr><td>mean_residual_deviance</td>\n",
       "<td>24086.396</td>\n",
       "<td>933.9532</td>\n",
       "<td>23746.18</td>\n",
       "<td>24667.213</td>\n",
       "<td>24806.812</td>\n",
       "<td>22592.182</td>\n",
       "<td>24619.598</td></tr>\n",
       "<tr><td>mse</td>\n",
       "<td>24086.396</td>\n",
       "<td>933.9532</td>\n",
       "<td>23746.18</td>\n",
       "<td>24667.213</td>\n",
       "<td>24806.812</td>\n",
       "<td>22592.182</td>\n",
       "<td>24619.598</td></tr>\n",
       "<tr><td>null_deviance</td>\n",
       "<td>372008768.0000000</td>\n",
       "<td>6977288.5</td>\n",
       "<td>383281920.0000000</td>\n",
       "<td>371502848.0000000</td>\n",
       "<td>365706912.0000000</td>\n",
       "<td>366800608.0000000</td>\n",
       "<td>372751520.0000000</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.6876925</td>\n",
       "<td>0.0134161</td>\n",
       "<td>0.6999237</td>\n",
       "<td>0.6846483</td>\n",
       "<td>0.6703911</td>\n",
       "<td>0.7024462</td>\n",
       "<td>0.6810528</td></tr>\n",
       "<tr><td>residual_deviance</td>\n",
       "<td>116093624.0000000</td>\n",
       "<td>4391832.5</td>\n",
       "<td>115002752.0000000</td>\n",
       "<td>117045928.0000000</td>\n",
       "<td>120536296.0000000</td>\n",
       "<td>109142832.0000000</td>\n",
       "<td>118740320.0000000</td></tr>\n",
       "<tr><td>rmse</td>\n",
       "<td>155.17421</td>\n",
       "<td>3.033766</td>\n",
       "<td>154.09796</td>\n",
       "<td>157.058</td>\n",
       "<td>157.50179</td>\n",
       "<td>150.30696</td>\n",
       "<td>156.90634</td></tr>\n",
       "<tr><td>rmsle</td>\n",
       "<td>0.1870964</td>\n",
       "<td>0.0187109</td>\n",
       "<td>0.1781198</td>\n",
       "<td>0.2199601</td>\n",
       "<td>0.1847949</td>\n",
       "<td>0.1754289</td>\n",
       "<td>0.1771784</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
      ],
      "text/plain": [
       "Model Details\n",
       "=============\n",
       "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
       "Model Key: StackedEnsemble_AllModels_5_AutoML_1_20241112_220908\n",
       "\n",
       "\n",
       "Model Summary for Stacked Ensemble: \n",
       "key                                   value\n",
       "------------------------------------  ----------------\n",
       "Stacking strategy                     cross_validation\n",
       "Number of base models (used / total)  20/44\n",
       "# GBM base models (used / total)      18/36\n",
       "# XGBoost base models (used / total)  2/5\n",
       "# GLM base models (used / total)      0/1\n",
       "# DRF base models (used / total)      0/2\n",
       "Metalearner algorithm                 GLM\n",
       "Metalearner fold assignment scheme    Random\n",
       "Metalearner nfolds                    5\n",
       "Metalearner fold_column\n",
       "Custom metalearner hyperparameters    None\n",
       "\n",
       "ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 21729.345578635894\n",
       "RMSE: 147.40877035860484\n",
       "MAE: 113.87901979976012\n",
       "RMSLE: 0.16583606754264976\n",
       "Mean Residual Deviance: 21729.345578635894\n",
       "R^2: 0.7163470681389101\n",
       "Null degrees of freedom: 10018\n",
       "Residual degrees of freedom: 9998\n",
       "Null deviance: 767543465.1646143\n",
       "Residual deviance: 217706313.352353\n",
       "AIC: 128530.62185618874\n",
       "\n",
       "ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on cross-validation data. **\n",
       "\n",
       "MSE: 24087.4315931657\n",
       "RMSE: 155.20126157079298\n",
       "MAE: 119.35785763757742\n",
       "RMSLE: 0.18768092487335972\n",
       "Mean Residual Deviance: 24087.4315931657\n",
       "R^2: 0.6878424037257309\n",
       "Null degrees of freedom: 24100\n",
       "Residual degrees of freedom: 24081\n",
       "Null deviance: 1860043823.2859292\n",
       "Residual deviance: 580531188.8268865\n",
       "AIC: 311603.400517189\n",
       "\n",
       "Cross-Validation Metrics Summary: \n",
       "                        mean         sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
       "----------------------  -----------  -----------  ------------  ------------  ------------  ------------  ------------\n",
       "aic                     62353.6      558.676      62579.9       61501         63002.8       62186.3       62497.9\n",
       "loglikelihood           0            0            0             0             0             0             0\n",
       "mae                     119.417      2.0314       119.013       120.539       120.697       116.025       120.811\n",
       "mean_residual_deviance  24086.4      933.953      23746.2       24667.2       24806.8       22592.2       24619.6\n",
       "mse                     24086.4      933.953      23746.2       24667.2       24806.8       22592.2       24619.6\n",
       "null_deviance           3.72009e+08  6.97729e+06  3.83282e+08   3.71503e+08   3.65707e+08   3.66801e+08   3.72752e+08\n",
       "r2                      0.687692     0.0134161    0.699924      0.684648      0.670391      0.702446      0.681053\n",
       "residual_deviance       1.16094e+08  4.39183e+06  1.15003e+08   1.17046e+08   1.20536e+08   1.09143e+08   1.1874e+08\n",
       "rmse                    155.174      3.03377      154.098       157.058       157.502       150.307       156.906\n",
       "rmsle                   0.187096     0.0187109    0.17812       0.21996       0.184795      0.175429      0.177178\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pandas DataFrame to H2O Frame\n",
    "data = h2o.H2OFrame(athletes_df)\n",
    "\n",
    "# Define feature columns and target variable\n",
    "feature_columns = [\n",
    "    'age', 'height', 'weight', 'experience_start_with_coach',\n",
    "    'experience_have_certificate', 'eat_on_diet', 'gender'\n",
    "]\n",
    "target_column = 'total_lift'\n",
    "\n",
    "# Ensure the target variable is numeric\n",
    "data[target_column] = data[target_column].asnumeric()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train, test = data.split_frame(ratios=[0.8], seed=42)\n",
    "\n",
    "# Run H2O AutoML with all features\n",
    "aml = H2OAutoML(\n",
    "    max_runtime_secs=360,      # Total time in seconds\n",
    "    seed=42,\n",
    "    exclude_algos=[\"DeepLearning\"]  # Exclude if GPU not available\n",
    ")\n",
    "\n",
    "print(\"Running H2O AutoML with all features...\")\n",
    "aml.train(\n",
    "    x=feature_columns,\n",
    "    y=target_column,\n",
    "    training_frame=train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "708fb1a7e2df0336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:36:09.759003Z",
     "start_time": "2024-11-13T05:36:09.715024Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AutoML Leaderboard (All Features):\n",
      "model_id                                                             rmse      mse      mae     rmsle    mean_residual_deviance\n",
      "StackedEnsemble_AllModels_5_AutoML_1_20241112_220908              155.201  24087.4  119.358  0.187681                   24087.4\n",
      "StackedEnsemble_BestOfFamily_6_AutoML_1_20241112_220908           155.322  24124.8  119.463  0.187872                   24124.8\n",
      "StackedEnsemble_BestOfFamily_4_AutoML_1_20241112_220908           155.413  24153.2  119.527  0.187955                   24153.2\n",
      "StackedEnsemble_Best1000_1_AutoML_1_20241112_220908               155.415  24153.8  119.471  0.187911                   24153.8\n",
      "StackedEnsemble_AllModels_3_AutoML_1_20241112_220908              155.434  24159.8  119.531  0.187948                   24159.8\n",
      "StackedEnsemble_BestOfFamily_3_AutoML_1_20241112_220908           155.459  24167.6  119.566  0.187872                   24167.6\n",
      "StackedEnsemble_AllModels_2_AutoML_1_20241112_220908              155.481  24174.4  119.589  0.187886                   24174.4\n",
      "GBM_lr_annealing_selection_AutoML_1_20241112_220908_select_model  155.523  24187.4  119.604  0.188105                   24187.4\n",
      "GBM_grid_1_AutoML_1_20241112_220908_model_1                       155.531  24190    119.652  0.188121                   24190\n",
      "GBM_5_AutoML_1_20241112_220908                                    155.555  24197.5  119.659  0.188029                   24197.5\n",
      "[10 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# View the AutoML leaderboard\n",
    "leaderboard = aml.leaderboard\n",
    "print(\"\\nAutoML Leaderboard (All Features):\")\n",
    "print(leaderboard.head(rows=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4bdf762b11f7c3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "372c6e7892a8d95e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:37:37.114512Z",
     "start_time": "2024-11-13T05:37:36.993684Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance on Test Data:\n",
      "ModelMetricsRegressionGLM: stackedensemble\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 23837.096097979236\n",
      "RMSE: 154.39266853701065\n",
      "MAE: 119.28531397122806\n",
      "RMSLE: 0.17650166157052083\n",
      "Mean Residual Deviance: 23837.096097979236\n",
      "R^2: 0.6887005254128638\n",
      "Null degrees of freedom: 5913\n",
      "Residual degrees of freedom: 5893\n",
      "Null deviance: 452858790.6387163\n",
      "Residual deviance: 140972586.3234492\n",
      "AIC: 76434.40095118352\n",
      "\n",
      "Calculating feature importances...\n",
      "Feature importance not available for the top model.\n",
      "\n",
      "Using all features as top features.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance on test data\n",
    "perf = aml.leader.model_performance(test_data=test)\n",
    "print(\"\\nModel Performance on Test Data:\")\n",
    "print(perf)\n",
    "\n",
    "# Get feature importance for the leader model\n",
    "print(\"\\nCalculating feature importances...\")\n",
    "if 'variable_importances' in aml.leader._model_json['output']:\n",
    "    importances = aml.leader.varimp(use_pandas=True)\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    print(importances)\n",
    "else:\n",
    "    print(\"Feature importance not available for the top model.\")\n",
    "\n",
    "# Get top 5 features\n",
    "if 'importances' in locals():\n",
    "    top_5_features = importances['variable'][:5].tolist()\n",
    "    print(f\"\\nTop 5 Features: {top_5_features}\")\n",
    "else:\n",
    "    # If feature importance is not available, use all features\n",
    "    top_5_features = feature_columns\n",
    "    print(\"\\nUsing all features as top features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcdc612eb0b89b9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53acc458f140f03e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:45:15.844576Z",
     "start_time": "2024-11-13T05:40:03.498272Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Top 3 Features: ['age', 'height', 'weight']\n",
      "\n",
      "Running H2O AutoML with top 3 features...\n",
      "AutoML progress: || (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
       "=============\n",
       "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
       "Model Key: StackedEnsemble_AllModels_5_AutoML_2_20241112_222002\n",
       "</pre>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-4.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-4 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-4 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-4 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-4 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-4 .h2o-table th,\n",
       "#h2o-table-4 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-4 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-4\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Model Summary for Stacked Ensemble: </caption>\n",
       "    <thead><tr><th>key</th>\n",
       "<th>value</th></tr></thead>\n",
       "    <tbody><tr><td>Stacking strategy</td>\n",
       "<td>cross_validation</td></tr>\n",
       "<tr><td>Number of base models (used / total)</td>\n",
       "<td>14/44</td></tr>\n",
       "<tr><td># GBM base models (used / total)</td>\n",
       "<td>12/36</td></tr>\n",
       "<tr><td># XGBoost base models (used / total)</td>\n",
       "<td>2/5</td></tr>\n",
       "<tr><td># DRF base models (used / total)</td>\n",
       "<td>0/2</td></tr>\n",
       "<tr><td># GLM base models (used / total)</td>\n",
       "<td>0/1</td></tr>\n",
       "<tr><td>Metalearner algorithm</td>\n",
       "<td>GLM</td></tr>\n",
       "<tr><td>Metalearner fold assignment scheme</td>\n",
       "<td>Random</td></tr>\n",
       "<tr><td>Metalearner nfolds</td>\n",
       "<td>5</td></tr>\n",
       "<tr><td>Metalearner fold_column</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>Custom metalearner hyperparameters</td>\n",
       "<td>None</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 32512.170445834596\n",
       "RMSE: 180.31131535717495\n",
       "MAE: 139.879439241216\n",
       "RMSLE: 0.20800360986605387\n",
       "Mean Residual Deviance: 32512.170445834596\n",
       "R^2: 0.5741634470550584\n",
       "Null degrees of freedom: 10019\n",
       "Residual degrees of freedom: 10005\n",
       "Null deviance: 765016731.9437269\n",
       "Residual deviance: 325771947.86726266\n",
       "AIC: 132569.01333565035</pre></div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on cross-validation data. **\n",
       "\n",
       "MSE: 34831.06667972926\n",
       "RMSE: 186.63082992830863\n",
       "MAE: 144.4431657183924\n",
       "RMSLE: 0.219860644129272\n",
       "Mean Residual Deviance: 34831.06667972926\n",
       "R^2: 0.548611814075769\n",
       "Null degrees of freedom: 24100\n",
       "Residual degrees of freedom: 24085\n",
       "Null deviance: 1860016643.6769476\n",
       "Residual deviance: 839463538.0481548\n",
       "AIC: 320484.3196582923</pre></div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-5.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-5 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-5 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-5 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table th,\n",
       "#h2o-table-5 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Cross-Validation Metrics Summary: </caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>mean</th>\n",
       "<th>sd</th>\n",
       "<th>cv_1_valid</th>\n",
       "<th>cv_2_valid</th>\n",
       "<th>cv_3_valid</th>\n",
       "<th>cv_4_valid</th>\n",
       "<th>cv_5_valid</th></tr></thead>\n",
       "    <tbody><tr><td>aic</td>\n",
       "<td>64122.35</td>\n",
       "<td>604.31805</td>\n",
       "<td>64370.71</td>\n",
       "<td>63130.195</td>\n",
       "<td>64754.97</td>\n",
       "<td>64113.29</td>\n",
       "<td>64242.59</td></tr>\n",
       "<tr><td>loglikelihood</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>mae</td>\n",
       "<td>144.43552</td>\n",
       "<td>1.4548091</td>\n",
       "<td>143.61261</td>\n",
       "<td>143.95007</td>\n",
       "<td>146.1337</td>\n",
       "<td>142.72513</td>\n",
       "<td>145.75604</td></tr>\n",
       "<tr><td>mean_residual_deviance</td>\n",
       "<td>34829.91</td>\n",
       "<td>716.8769</td>\n",
       "<td>34427.03</td>\n",
       "<td>34875.15</td>\n",
       "<td>35665.38</td>\n",
       "<td>33856.445</td>\n",
       "<td>35325.54</td></tr>\n",
       "<tr><td>mse</td>\n",
       "<td>34829.91</td>\n",
       "<td>716.8769</td>\n",
       "<td>34427.03</td>\n",
       "<td>34875.15</td>\n",
       "<td>35665.38</td>\n",
       "<td>33856.445</td>\n",
       "<td>35325.54</td></tr>\n",
       "<tr><td>null_deviance</td>\n",
       "<td>372003328.0000000</td>\n",
       "<td>6143969.0</td>\n",
       "<td>376695072.0000000</td>\n",
       "<td>363077920.0000000</td>\n",
       "<td>370424448.0000000</td>\n",
       "<td>371041024.0000000</td>\n",
       "<td>378778208.0000000</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.5484429</td>\n",
       "<td>0.0110087</td>\n",
       "<td>0.5573731</td>\n",
       "<td>0.5439957</td>\n",
       "<td>0.5320914</td>\n",
       "<td>0.5592475</td>\n",
       "<td>0.5495068</td></tr>\n",
       "<tr><td>residual_deviance</td>\n",
       "<td>167889568.0000000</td>\n",
       "<td>3930004.5</td>\n",
       "<td>166730112.0000000</td>\n",
       "<td>165482576.0000000</td>\n",
       "<td>173298080.0000000</td>\n",
       "<td>163526624.0000000</td>\n",
       "<td>170410400.0000000</td></tr>\n",
       "<tr><td>rmse</td>\n",
       "<td>186.61981</td>\n",
       "<td>1.9226604</td>\n",
       "<td>185.54523</td>\n",
       "<td>186.74889</td>\n",
       "<td>188.8528</td>\n",
       "<td>184.0012</td>\n",
       "<td>187.9509</td></tr>\n",
       "<tr><td>rmsle</td>\n",
       "<td>0.2196389</td>\n",
       "<td>0.0127611</td>\n",
       "<td>0.2109691</td>\n",
       "<td>0.2394518</td>\n",
       "<td>0.2185897</td>\n",
       "<td>0.2064797</td>\n",
       "<td>0.2227042</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
      ],
      "text/plain": [
       "Model Details\n",
       "=============\n",
       "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
       "Model Key: StackedEnsemble_AllModels_5_AutoML_2_20241112_222002\n",
       "\n",
       "\n",
       "Model Summary for Stacked Ensemble: \n",
       "key                                   value\n",
       "------------------------------------  ----------------\n",
       "Stacking strategy                     cross_validation\n",
       "Number of base models (used / total)  14/44\n",
       "# GBM base models (used / total)      12/36\n",
       "# XGBoost base models (used / total)  2/5\n",
       "# DRF base models (used / total)      0/2\n",
       "# GLM base models (used / total)      0/1\n",
       "Metalearner algorithm                 GLM\n",
       "Metalearner fold assignment scheme    Random\n",
       "Metalearner nfolds                    5\n",
       "Metalearner fold_column\n",
       "Custom metalearner hyperparameters    None\n",
       "\n",
       "ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 32512.170445834596\n",
       "RMSE: 180.31131535717495\n",
       "MAE: 139.879439241216\n",
       "RMSLE: 0.20800360986605387\n",
       "Mean Residual Deviance: 32512.170445834596\n",
       "R^2: 0.5741634470550584\n",
       "Null degrees of freedom: 10019\n",
       "Residual degrees of freedom: 10005\n",
       "Null deviance: 765016731.9437269\n",
       "Residual deviance: 325771947.86726266\n",
       "AIC: 132569.01333565035\n",
       "\n",
       "ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on cross-validation data. **\n",
       "\n",
       "MSE: 34831.06667972926\n",
       "RMSE: 186.63082992830863\n",
       "MAE: 144.4431657183924\n",
       "RMSLE: 0.219860644129272\n",
       "Mean Residual Deviance: 34831.06667972926\n",
       "R^2: 0.548611814075769\n",
       "Null degrees of freedom: 24100\n",
       "Residual degrees of freedom: 24085\n",
       "Null deviance: 1860016643.6769476\n",
       "Residual deviance: 839463538.0481548\n",
       "AIC: 320484.3196582923\n",
       "\n",
       "Cross-Validation Metrics Summary: \n",
       "                        mean         sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
       "----------------------  -----------  -----------  ------------  ------------  ------------  ------------  ------------\n",
       "aic                     64122.3      604.318      64370.7       63130.2       64755         64113.3       64242.6\n",
       "loglikelihood           0            0            0             0             0             0             0\n",
       "mae                     144.436      1.45481      143.613       143.95        146.134       142.725       145.756\n",
       "mean_residual_deviance  34829.9      716.877      34427         34875.2       35665.4       33856.4       35325.5\n",
       "mse                     34829.9      716.877      34427         34875.2       35665.4       33856.4       35325.5\n",
       "null_deviance           3.72003e+08  6.14397e+06  3.76695e+08   3.63078e+08   3.70424e+08   3.71041e+08   3.78778e+08\n",
       "r2                      0.548443     0.0110087    0.557373      0.543996      0.532091      0.559248      0.549507\n",
       "residual_deviance       1.6789e+08   3.93e+06     1.6673e+08    1.65483e+08   1.73298e+08   1.63527e+08   1.7041e+08\n",
       "rmse                    186.62       1.92266      185.545       186.749       188.853       184.001       187.951\n",
       "rmsle                   0.219639     0.0127611    0.210969      0.239452      0.21859       0.20648       0.222704\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data using only the top 3 features\n",
    "top_3_features = top_5_features[:3]\n",
    "print(f\"Using Top 3 Features: {top_3_features}\")\n",
    "\n",
    "# Run H2O AutoML with top 3 features\n",
    "aml_top3 = H2OAutoML(\n",
    "    max_runtime_secs=360,\n",
    "    seed=42,\n",
    "    exclude_algos=[\"DeepLearning\"]  # Exclude if GPU not available\n",
    ")\n",
    "\n",
    "print(\"\\nRunning H2O AutoML with top 3 features...\")\n",
    "aml_top3.train(\n",
    "    x=top_3_features,\n",
    "    y=target_column,\n",
    "    training_frame=train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b542fa31cafae0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:45:23.580807Z",
     "start_time": "2024-11-13T05:45:23.532831Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AutoML Leaderboard (Top 3 Features):\n",
      "model_id                                                    rmse      mse      mae     rmsle    mean_residual_deviance\n",
      "StackedEnsemble_AllModels_5_AutoML_2_20241112_222002     186.631  34831.1  144.443  0.219861                   34831.1\n",
      "StackedEnsemble_AllModels_3_AutoML_2_20241112_222002     186.848  34912.3  144.578  0.219979                   34912.3\n",
      "StackedEnsemble_Best1000_1_AutoML_2_20241112_222002      186.855  34914.9  144.576  0.219978                   34914.9\n",
      "StackedEnsemble_BestOfFamily_5_AutoML_2_20241112_222002  186.927  34941.7  144.729  0.220143                   34941.7\n",
      "StackedEnsemble_BestOfFamily_3_AutoML_2_20241112_222002  186.95   34950.3  144.706  0.2201                     34950.3\n",
      "StackedEnsemble_AllModels_2_AutoML_2_20241112_222002     186.954  34951.7  144.705  0.220092                   34951.7\n",
      "GBM_5_AutoML_2_20241112_222002                           186.971  34958    144.781  0.220182                   34958\n",
      "GBM_grid_1_AutoML_2_20241112_222002_model_1              187.029  34980    144.663  0.220272                   34980\n",
      "GBM_grid_1_AutoML_2_20241112_222002_model_81             187.078  34998.3  144.832  0.220459                   34998.3\n",
      "GBM_2_AutoML_2_20241112_222002                           187.131  35017.9  144.915  0.220216                   35017.9\n",
      "[10 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# View the AutoML leaderboard for top 3 features\n",
    "leaderboard_top3 = aml_top3.leaderboard\n",
    "print(\"\\nAutoML Leaderboard (Top 3 Features):\")\n",
    "print(leaderboard_top3.head(rows=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1589618904da586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T05:45:29.798490Z",
     "start_time": "2024-11-13T05:45:29.717218Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance on Test Data (Top 3 Features):\n",
      "ModelMetricsRegressionGLM: stackedensemble\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 35092.30653892611\n",
      "RMSE: 187.3294064980886\n",
      "MAE: 145.59100384998297\n",
      "RMSLE: 0.21320004931182612\n",
      "Mean Residual Deviance: 35092.30653892611\n",
      "R^2: 0.5417136155043435\n",
      "Null degrees of freedom: 5913\n",
      "Residual degrees of freedom: 5899\n",
      "Null deviance: 452858790.6387163\n",
      "Residual deviance: 207535900.871209\n",
      "AIC: 78709.5747626964\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance on test data with top 3 features\n",
    "perf_top3 = aml_top3.leader.model_performance(test_data=test)\n",
    "print(\"\\nModel Performance on Test Data (Top 3 Features):\")\n",
    "print(perf_top3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
